{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d10ff-b5f9-455d-bf24-59c0be4e5695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T19:11:45.308530Z",
     "iopub.status.busy": "2024-02-22T19:11:45.308010Z",
     "iopub.status.idle": "2024-02-22T19:12:00.476883Z",
     "shell.execute_reply": "2024-02-22T19:12:00.476471Z",
     "shell.execute_reply.started": "2024-02-22T19:11:45.308506Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_unixtime, year, month, day, col, to_timestamp\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "awsRegion = \"\"\n",
    "glueDatabaseName = \"\"\n",
    "glueTableName = \"\"\n",
    "\n",
    "sourceDirectory = \"s3://\" # raw, unpartitioned data\n",
    "destinationDirectory = \"s3://\" # final partitioned resting space\n",
    "\n",
    "# Epochseconds column name -- if you are using another date/time format ensure you change the SQL operator for parsing\n",
    "epochColumn = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2402dac-f9a1-4940-ae23-e8afe3cc2944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T19:25:20.281530Z",
     "iopub.status.busy": "2024-02-22T19:25:20.281050Z",
     "iopub.status.idle": "2024-02-22T19:25:25.789048Z",
     "shell.execute_reply": "2024-02-22T19:25:25.788596Z",
     "shell.execute_reply.started": "2024-02-22T19:25:20.281505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load DF, convert Epochseconds to timestamp\n",
    "df = spark.read.format(\"parquet\").option(\"compression\", \"zstd\").load(sourceDirectory)\n",
    "df = df.withColumn(epochColumn, from_unixtime(epochColumn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da4752-1ba8-4817-a173-ef56a2c16cec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T19:25:29.213098Z",
     "iopub.status.busy": "2024-02-22T19:25:29.212587Z",
     "iopub.status.idle": "2024-02-22T19:25:29.584709Z",
     "shell.execute_reply": "2024-02-22T19:25:29.584248Z",
     "shell.execute_reply.started": "2024-02-22T19:25:29.213076Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract year, month, and day from epoch seconds column for partitioning\n",
    "df = df.withColumn(\"Year\", year(epochColumn))\n",
    "df = df.withColumn(\"Month\", month(epochColumn))\n",
    "df = df.withColumn(\"Day\", day(epochColumn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cccdf-e78e-4ddc-8ea5-1dd4a35b093a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T19:25:35.820286Z",
     "iopub.status.busy": "2024-02-22T19:25:35.820018Z",
     "iopub.status.idle": "2024-02-22T19:31:27.828078Z",
     "shell.execute_reply": "2024-02-22T19:31:27.827684Z",
     "shell.execute_reply.started": "2024-02-22T19:25:35.820266Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# ensure that converted time column is cast as a timestamp correctly\n",
    "df = df.withColumn(epochColumn, to_timestamp(epochColumn).cast(TimestampType()))\n",
    "\n",
    "# write out to destination in append mode\n",
    "df.write.partitionBy(\"Year\", \"Month\", \"Day\").format(\"parquet\").option(\"compression\", \"zstd\").mode(\"append\").save(destinationDirectory)\n",
    "\n",
    "# read the unique partitions\n",
    "df = spark.read.format(\"parquet\").load(destinationDirectory)\n",
    "partitions = df.select(\"Year\", \"Month\", \"Day\").distinct().collect()\n",
    "\n",
    "# read the schema\n",
    "schema = df.schema\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76cc37-ee37-4d9c-95a9-857702e35267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T19:58:07.553417Z",
     "iopub.status.busy": "2024-02-22T19:58:07.553151Z",
     "iopub.status.idle": "2024-02-22T19:58:07.714971Z",
     "shell.execute_reply": "2024-02-22T19:58:07.714578Z",
     "shell.execute_reply.started": "2024-02-22T19:58:07.553397Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup deps for creating Glue Table\n",
    "import boto3\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "glue = boto3.client(\"glue\", region_name=awsRegion)\n",
    "\n",
    "# convert Spark DF types -> Athena engine v3 (Trino-ish?) types\n",
    "def sparkDataTypeToAthenaDataType(sparkDataType):\n",
    "    mapping = {\n",
    "        IntegerType: \"int\",\n",
    "        LongType: \"bigint\",\n",
    "        DoubleType: \"double\",\n",
    "        FloatType: \"float\",\n",
    "        StringType: \"string\",\n",
    "        BooleanType: \"boolean\",\n",
    "        DateType: \"date\",\n",
    "        TimestampType: \"timestamp\",\n",
    "    }\n",
    "    return mapping.get(type(sparkDataType), \"string\")  # Default to string type if unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getGlueTableColumns(schema, partitionKeys):\n",
    "    columns = []\n",
    "\n",
    "    for field in schema.fields:\n",
    "        if field.name not in partitionKeys:  # Skip partition keys\n",
    "            athenaDataType = sparkDataTypeToAthenaDataType(field.dataType)\n",
    "            columns.append({\"Name\": field.name, \"Type\": athenaDataType})\n",
    "    \n",
    "    return columns\n",
    "\n",
    "def createGlueTable(glueDatabaseName, glueTableName, columns, partitionKeys, destinationDirectory):\n",
    "    glue.create_table(\n",
    "        DatabaseName=glueDatabaseName,\n",
    "        TableInput={\n",
    "            \"Name\": glueTableName,\n",
    "            \"StorageDescriptor\": {\n",
    "                \"Columns\": columns,\n",
    "                \"Location\": destinationDirectory,\n",
    "                \"InputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "                \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "                \"SerdeInfo\": {\n",
    "                    \"SerializationLibrary\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\n",
    "                },\n",
    "            },\n",
    "            \"PartitionKeys\": [{\"Name\": key, \"Type\": \"string\"} for key in partitionKeys],\n",
    "            \"TableType\": \"EXTERNAL_TABLE\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# get the columns, sans partitions\n",
    "partitionKeys = [\"Year\", \"Month\", \"Day\"]\n",
    "columns = getGlueTableColumns(schema, partitionKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8de7f1-8bfa-4975-a9fa-9f22aa7e3360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T19:58:10.861220Z",
     "iopub.status.busy": "2024-02-22T19:58:10.860820Z",
     "iopub.status.idle": "2024-02-22T19:58:11.215606Z",
     "shell.execute_reply": "2024-02-22T19:58:11.215171Z",
     "shell.execute_reply.started": "2024-02-22T19:58:10.861201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    c = createGlueTable(glueDatabaseName, glueTableName, columns, partitionKeys, destinationDirectory)\n",
    "    print(\"table create successfully\")\n",
    "    print(c)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf328730-c2b8-4a48-b5ba-502a5b88976c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T19:58:44.312998Z",
     "iopub.status.busy": "2024-02-22T19:58:44.312722Z",
     "iopub.status.idle": "2024-02-22T19:58:45.210167Z",
     "shell.execute_reply": "2024-02-22T19:58:45.209743Z",
     "shell.execute_reply.started": "2024-02-22T19:58:44.312978Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def addPartitionsToTable(glueDatabaseName, glueTableName, partitions, destinationDirectory):\n",
    "    partitionInputs = []\n",
    "    for partition in partitions:\n",
    "        year, month, day = partition[\"Year\"], partition[\"Month\"], partition[\"Day\"]\n",
    "\n",
    "        # Construct the s3 uri for this specific partition\n",
    "        partitionLocation = f\"{destinationDirectory}/Year={year}/Month={month}/Day={day}\"\n",
    "        partitionInput = {\n",
    "            \"Values\": [str(year), str(month), str(day)],\n",
    "            \"StorageDescriptor\": {\n",
    "                \"Columns\": [],  # This can be empty as columns are defined at the table level\n",
    "                \"Location\": partitionLocation,\n",
    "                \"InputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "                \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "                \"SerdeInfo\": {\n",
    "                    \"SerializationLibrary\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        partitionInputs.append(partitionInput)\n",
    "\n",
    "    # Use the AWS Glue client to batch create partitions\n",
    "    try:\n",
    "        glue.batch_create_partition(\n",
    "            DatabaseName=glueDatabaseName,\n",
    "            TableName=glueTableName,\n",
    "            PartitionInputList=partitionInputs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def createUniqueChunks(data, maxPartitionCount=95):\n",
    "    unique_data = list(set(data))  # Remove duplicates to ensure uniqueness\n",
    "    chunks = [unique_data[i:i + maxPartitionCount] for i in range(0, len(unique_data), maxPartitionCount)]\n",
    "    return chunks\n",
    "\n",
    "def partitionAndProcess(data):\n",
    "    if len(data) > 95:\n",
    "        chunks = createUniqueChunks(data)\n",
    "        for chunk in chunks:\n",
    "            addPartitionsToTable(glueDatabaseName, glueTableName, chunk, destinationDirectory)\n",
    "    else:\n",
    "        process_chunk(data)\n",
    "\n",
    "# Using the partitions collected from your DataFrame - split them if there are more than 95 and bulk add the data to the table\n",
    "partitionChunk = partitionAndProcess(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5f664-49eb-4862-ba5e-3cd61adda8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "spark_magic_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
